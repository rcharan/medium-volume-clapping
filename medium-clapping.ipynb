{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   utilities import load_config, get_browser\n",
    "import itertools\n",
    "import more_itertools\n",
    "from   more_itertools import unique_everseen as uniq\n",
    "import re\n",
    "from   itertools import chain\n",
    "from   numpy.random import randn as gauss_noise\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, NoSuchElementException, NoSuchWindowException, ElementClickInterceptedException, WebDriverException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get links to scrape from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the API and load the configuration file\n",
    "from googleapiclient.discovery import build\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parsing strategy will be brute force\n",
    "#  The return is just a list of values anywhere in the \n",
    "#  heirarchy of lists/dicts returned by the sheets API\n",
    "def flatten(data):\n",
    "    if isinstance(data, dict):\n",
    "        contents = list(data.values())\n",
    "    elif isinstance(data, list):\n",
    "        contents = data\n",
    "    else:\n",
    "        return [data]\n",
    "    \n",
    "    return chain.from_iterable([flatten(v) for v in contents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a regex to extract URLs from the sheet\n",
    "def parse_medium_articles(datum):\n",
    "    # Things that are not strings are not URLs\n",
    "    if not isinstance(datum, str):\n",
    "        return None\n",
    "    \n",
    "    # URLs are strings starting with http\n",
    "    url = re.search('''(http[a-z%A-Z0-9\\-_@\\./:]*)''', datum)\n",
    "    if not url:\n",
    "        if datum.find('http') != -1:\n",
    "            breakpoint()\n",
    "        return None\n",
    "    else:\n",
    "        url = url.group(1)\n",
    "    \n",
    "    # Bad URLs are those with a name followed by nothing\n",
    "    #  This is the case if someone links to their medium profile and \n",
    "    #  not literal articles\n",
    "    #   Future to do: fix this\n",
    "    not_an_article = re.search('medium.com/@[^/]*(/$|$)', url)\n",
    "    if not_an_article:\n",
    "        return None\n",
    "\n",
    "    # Throw out URLs that are a reference to Google Docs\n",
    "    is_google = re.search('.google.com/', url)\n",
    "    if is_google:\n",
    "        return None\n",
    "    \n",
    "    return url\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Google Sheets api with return reponse :: dict\n",
    "service        = build('sheets', 'v4', developerKey=config['api_key'])\n",
    "spreadsheet_id = config['spreadsheet_id']\n",
    "request        = service.spreadsheets().get(spreadsheetId=spreadsheet_id, includeGridData = True)\n",
    "response       = request.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_detected = list(uniq(filter(None, map(parse_medium_articles, flatten(response)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path('clapped_urls.txt').exists():\n",
    "    clapped_urls = []\n",
    "else:\n",
    "    with open('clapped_urls.txt', 'r') as f:\n",
    "        clapped_urls = f.readlines()\n",
    "        clapped_urls = list(map(lambda s : s.strip(), clapped_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clap = list(set(links_detected).difference(clapped_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the actual clapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xpath(child = None):\n",
    "    button_text_reqd = False\n",
    "    animation_div = '''//div[contains(@style, 'animation')]'''\n",
    "    button_by_svg_size = '''//button[./*[name()='svg' and @width=33]]'''\n",
    "    \n",
    "    if child == 'svg':\n",
    "        return button_by_svg_size\n",
    "\n",
    "    if child == 'button':\n",
    "        child = '//button'\n",
    "        button_text_reqd = True\n",
    "    elif child == 'animation':\n",
    "        child = animation_div\n",
    "    else:\n",
    "        child = ''\n",
    "\n",
    "    div_class     = '''(@class = \"n o\")'''\n",
    "    button        = '''.//button''' + ('''[contains(text(), 'clap')]''' if button_text_reqd else '')\n",
    "    \n",
    "    \n",
    "    return f'//div[{div_class} and {button}]' + child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xpath_with_waiting(browser, time_out = 2, poll_frequency = 0.5, child = None):\n",
    "    return WebDriverWait(browser, time_out, poll_frequency = poll_frequency).until(\n",
    "                      EC.presence_of_element_located(\n",
    "                        (By.XPATH, make_xpath(child = child)\n",
    "                      )\n",
    "                    )\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clap_for_url(browser, url, target_time = 3):\n",
    "    '''\n",
    "        Clicks on the 'clap' button on a given URL\n",
    "        \n",
    "        Attempts to find the clap button and clap on it. Attempts to \n",
    "        verify the success of the clapping and count the number already given.\n",
    "          - On success, proceed to clap up to the maximum of 50 times.\n",
    "            The wait time between claps is randomized to emulate human behavior\n",
    "            with a goal of the total approximate time for a full 50 clicks being the\n",
    "            target_time.\n",
    "          - On failure, attempt to provide a diagnosis to the user. Then return to \n",
    "            the main flow which will record the results and proceed to the next URL.\n",
    "    '''\n",
    "    browser.get(url)\n",
    "    \n",
    "    # Try to find the button for clapping. Wait up to 2 seconds after the page\n",
    "    #  loads for the contents of the page to load.\n",
    "    try:\n",
    "        clapper = get_xpath_with_waiting(browser, child = 'button')\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            clapper = get_xpath_with_waiting(browser, child = 'svg')\n",
    "            clapper.click()\n",
    "            clapper = get_xpath_with_waiting(browser, time_out = 5, child = 'button')\n",
    "        except TimeoutException:\n",
    "            print(f'Unable to find the clapping button on {url}.\\n'\n",
    "                   '  Possible issues:\\n'\n",
    "                   '  (1) Medium changed the internal structure of the page\\n'\n",
    "                   '  (2) Nobody has clapped on this article before\\n'\n",
    "                   '  (3) Page load was too slow.'\n",
    "                 )\n",
    "            breakpoint()\n",
    "            return False\n",
    "    \n",
    "    if clapper.text.find('claps') != -1:\n",
    "        print(f'This url {url} appears to be your own article. So it will not be clicked on.')\n",
    "        return True\n",
    "    \n",
    "    # Click on it once\n",
    "    num_fails = 0\n",
    "    max_fails = 10\n",
    "    success   = False\n",
    "    while num_fails < max_fails and not success:\n",
    "        try:\n",
    "            clapper.click()\n",
    "        except ElementClickInterceptedException:\n",
    "            time.sleep(1)\n",
    "            num_fails += 1\n",
    "        else:\n",
    "            success = True\n",
    "    if max_fails == num_fails:\n",
    "        print(f'Something stopped/intercepted the click on {url}')\n",
    "        return False\n",
    "    \n",
    "    # On success, the number of clicks already given will be displayed in an animation.\n",
    "    #  If so, collect that information\n",
    "    #  On failure to collect the information, assume that clicking has failed\n",
    "    try:\n",
    "        elt = WebDriverWait(browser, 1, poll_frequency = 0.1).until(\n",
    "                 EC.presence_of_element_located(\n",
    "                     (By.XPATH, make_xpath(child = 'animation'))\n",
    "                 )\n",
    "               )\n",
    "        num_clicks_already = int(elt.text)\n",
    "    except TimeoutException:\n",
    "        print(f'Something went wrong after clicking on the clapping button in {url}\\n'\n",
    "               '  Possible issues:\\n'\n",
    "               '  (1) You are not logged in\\n'\n",
    "               '  (2) This is your own article. You cannot clap for yourself.'\n",
    "                    ' After all, what is the sound of one hand clapping?\\n'\n",
    "             )\n",
    "        return False\n",
    "    \n",
    "    # The wait time is normalized to emulate human behavior\n",
    "    #  This is totally unneccesary but fun\n",
    "    def wait_time():\n",
    "        mu    =  target_time / 50\n",
    "        sigma =  mu / 5\n",
    "        return mu + gauss_noise() * sigma\n",
    "    \n",
    "    clicks_to_do = 50 - num_clicks_already\n",
    "    if clicks_to_do == 0:\n",
    "        print(f'This article is already maximally clapped! At url {url}')\n",
    "        return True\n",
    "    \n",
    "    while clicks_to_do > 0:\n",
    "        clapper.click()\n",
    "        clicks_to_do -= 1\n",
    "        time.sleep(wait_time())\n",
    "                \n",
    "    print(f'Succesfully clicked on {url}')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clap_for_urls(urls):\n",
    "    with get_browser() as browser, open('clapped_urls.txt', 'a') as success_file:\n",
    "        if not browser:\n",
    "            print('Program Terminating')\n",
    "            return\n",
    "        for url in urls:\n",
    "\n",
    "            # Pass the url to the clapping function. Clapping errors are handled\n",
    "            #  internally by the clapping function. Catch possible miscellaneous errors\n",
    "            #  and also user intent to terminate communicated by closing the browser.\n",
    "            try:\n",
    "                result = clap_for_url(browser, url)\n",
    "            except NoSuchWindowException:\n",
    "                print(f'It looks like you closed the window. Program will terminate')\n",
    "                return\n",
    "            except StaleElementReferenceException:\n",
    "                print(f'This is an unanticipated error on url {url}.\\n'\n",
    "                      'In the Rumsfeld classfication it '\n",
    "                      'is a known unknown. Please report this error to the authorities. '\n",
    "                      'Proceeding to the next url')\n",
    "                result = False\n",
    "\n",
    "            # Handle the result. True = success; False = failure \n",
    "            if result:\n",
    "                success_file.write(f'{url}\\n')\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                wait_time = 5\n",
    "                print(f'Program will wait {wait_time} seconds before proceeding to the next URL.'\n",
    "                       ' Close the medium.com window at any time to terminate.\\n')\n",
    "                time.sleep(wait_time)\n",
    "    \n",
    "        browser.get('https://medium.com')\n",
    "        time.sleep(10)\n",
    "    print('All Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
